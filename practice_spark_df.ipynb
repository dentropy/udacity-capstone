{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39c7a16e-b100-421b-ac2b-0f6743731300",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T\n",
    "import modules.parsers as parsers\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98cbeb06-d1f3-463f-9fcd-ec5461f701c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/07 21:52:25 WARN Utils: Your hostname, pop-os resolves to a loopback address: 127.0.1.1; using 10.10.64.126 instead (on interface wlp59s0)\n",
      "22/04/07 21:52:25 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/paul/.local/lib/python3.8/site-packages/pyspark/jars/spark-unsafe_2.12-3.2.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/04/07 21:52:25 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder\\\n",
    "                     .master(\"spark://pop-os.localdomain:7077\")\\\n",
    "                     .enableHiveSupport() \\\n",
    "                     .getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80499a69-3fc9-4608-93fc-f2a28eb379d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "# Extract a URL from a string\n",
    "\n",
    "## df_extract_url\n",
    "#  * Take input of a dataframe\n",
    "#  * Takes a column name to extract url from body of text\n",
    "# Regex Source: https://stackoverflow.com/questions/28185064/python-infinite-loop-in-regex-to-match-url\n",
    "def df_extract_url(tmp_df, tmp_col):\n",
    "    return tmp_df.withColumn(\"url\", F.regexp_extract(F.col(tmp_col), r\"(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\\\".,<>?«»“”‘’]))\", 0))          \n",
    "\n",
    "\n",
    "# Get domain name from URL and put it in domainname table\n",
    "# Regex Source: https://stackoverflow.com/questions/25703360/regular-expression-extract-subdomain-domain\n",
    "def df_parse_domainname(tmp_df, tmp_col):\n",
    "    tmp_df = tmp_df.withColumn( \"domainname\", F.regexp_extract(F.col(tmp_col) , r'^(?:http:\\/\\/|www\\.|https:\\/\\/)([^\\/]+)', 1)) \n",
    "    return tmp_df\n",
    "\n",
    "# Get the domain name of an email from an email address\n",
    "def df_parse_email(tmp_df, tmp_col):\n",
    "    tmp_df = tmp_df.withColumn(\"email_username\" , F.regexp_extract(F.col(tmp_col),   r'([^@]+)', 1)) \n",
    "    tmp_df = tmp_df.withColumn(\"email_domain\"   , F.regexp_extract(F.col(tmp_col) ,  r'@(.*)'  , 1)) \n",
    "    return tmp_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be7345f-3fd7-4dc1-8528-7fe5ec48bdec",
   "metadata": {},
   "source": [
    "## Working with git repos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c09890d-ea7e-46e5-9ccb-a424e72f4703",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "files = glob.glob('./out/git_out/**/*.json',  recursive=True)\n",
    "first_file = files.pop()\n",
    "git_repo_df = spark.read.json(first_file)\n",
    "git_repo_df = parsers.df_parse_email(git_repo_df,  \"author_email\")\n",
    "git_repo_df.first()\n",
    "for tmp_df_path in files:\n",
    "    tmp_git_repo_df = spark.read.json(tmp_df_path)\n",
    "    tmp_git_repo_df = parsers.df_parse_email(tmp_git_repo_df,  \"author_email\")\n",
    "    final_df = git_repo_df.unionByName(tmp_git_repo_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08a97ca1-2b09-4365-ad98-407258172e3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------+----------------+-------+------------------------------------+---------------------+------------------------+\n",
      "|author_email                                  |author_name     |commits|remote_url                          |email_username       |email_domain            |\n",
      "+----------------------------------------------+----------------+-------+------------------------------------+---------------------+------------------------+\n",
      "|0xfawkes@protonmail.com                       |0xfawkes        |1      |https://github.com/ethereum/EIPs.git|0xfawkes             |protonmail.com          |\n",
      "|12873030+mryalamanchi@users.noreply.github.com|mr.yalamanchi   |3      |https://github.com/ethereum/EIPs.git|12873030+mryalamanchi|users.noreply.github.com|\n",
      "|14004106+lightclient@users.noreply.github.com |lightclient     |62     |https://github.com/ethereum/EIPs.git|14004106+lightclient |users.noreply.github.com|\n",
      "|1591639+s1na@users.noreply.github.com         |Sina Mahmoodi   |2      |https://github.com/ethereum/EIPs.git|1591639+s1na         |users.noreply.github.com|\n",
      "|1641795+vikmeup@users.noreply.github.com      |Viktor Radchenko|1      |https://github.com/ethereum/EIPs.git|1641795+vikmeup      |users.noreply.github.com|\n",
      "|16929357+GregTheGreek@users.noreply.github.com|Gregory Markou  |7      |https://github.com/ethereum/EIPs.git|16929357+GregTheGreek|users.noreply.github.com|\n",
      "|16969914+jamesray1@users.noreply.github.com   |James Ray       |61     |https://github.com/ethereum/EIPs.git|16969914+jamesray1   |users.noreply.github.com|\n",
      "|17414704+VexyCats@users.noreply.github.com    |VexyCats        |2      |https://github.com/ethereum/EIPs.git|17414704+VexyCats    |users.noreply.github.com|\n",
      "|1797079433@qq.com                             |Zhihao Chen     |1      |https://github.com/ethereum/EIPs.git|1797079433           |qq.com                  |\n",
      "|22412996+zemse@users.noreply.github.com       |soham           |1      |https://github.com/ethereum/EIPs.git|22412996+zemse       |users.noreply.github.com|\n",
      "+----------------------------------------------+----------------+-------+------------------------------------+---------------------+------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_df.show(10, False) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fffd2a98-1d5f-428a-ae07-824aed92ed94",
   "metadata": {},
   "source": [
    "## Working with reddit data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "49bea437-dc98-457c-ba74-d943d2f8ed75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------+------------------------------+\n",
      "|url                                                         |domain                        |\n",
      "+------------------------------------------------------------+------------------------------+\n",
      "|https://auth.geeksforgeeks.org/user/Chinmoy%20Lenka/articles|https://auth.geeksforgeeks.org|\n",
      "|https://www.udacity.com/                                    |https://www.udacity.com       |\n",
      "|www.udacity.com                                             |www.udacity.com               |\n",
      "|                                                            |                              |\n",
      "+------------------------------------------------------------+------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text = ['My Profile: https://auth.geeksforgeeks.org/user/Chinmoy%20Lenka/articles in the portal of https://www.geeksforgeeks.org/',\n",
    "        \"Hello world https://www.udacity.com/\", \n",
    "        \"Hello World www.udacity.com\", \n",
    "        \"What about these URLs google.com NOPE\",  \n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(text, T.StringType()).toDF(\"text\")\n",
    "\n",
    "df = df.withColumn(\"url\", F.regexp_extract(df.text, r\"(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\\\".,<>?«»“”‘’]))\", 0))          \n",
    "df = df.withColumn(\"domain\", F.regexp_extract(df.url, r'^(?:http:\\/\\/|www\\.|https:\\/\\/)([^\\/]+)', 0))            \n",
    "df[[\"url\", \"domain\"]].show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "63ea5047-ff4b-451d-839e-153d3bc463d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_df = spark.read.json('./data/reddit-stuff/xaa')# RC_2018-01-01.ndjson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b89f968f-1a1b-4a07-b5d8-d0fddceccdf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_df = df_extract_url(reddit_df, \"body\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c0441b93-adb3-48c2-a5e5-5d11e0394869",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|                body|                 url|\n",
      "+--------------------+--------------------+\n",
      "|We agree and that...|https://pecuniare...|\n",
      "|We agree and that...|https://pecuniare...|\n",
      "|We agree and that...|https://pecuniare...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reddit_df[[\"body\", \"url\"]].filter( F.col(\"url\") != \"\" ).show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9468cb9a-7fc4-4a71-9e72-86add8fb8ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_df = parsers.df_parse_domainname(reddit_df, \"url\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a02bf2d6-3952-4a8a-bb53-1da36f49cba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+-------------------+\n",
      "|url                         |domainname         |\n",
      "+----------------------------+-------------------+\n",
      "|https://pecuniaresearch.com/|pecuniaresearch.com|\n",
      "|https://pecuniaresearch.com/|pecuniaresearch.com|\n",
      "|https://pecuniaresearch.com/|pecuniaresearch.com|\n",
      "|https://pecuniaresearch.com/|pecuniaresearch.com|\n",
      "|https://pecuniaresearch.com/|pecuniaresearch.com|\n",
      "+----------------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reddit_df[[\"url\", \"domainname\"]].filter( F.col(\"domainname\") != \"\" ).show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "babd5feb-d071-4d10-bbf9-100f53c43a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "domains_list = reddit_df[[\"domainname\"]].\\\n",
    "    withColumnRenamed(\"domainname\",\"domain_name\").\\\n",
    "    union(git_repo_df[[\"email_domain\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e49376e2-058b-4b5b-9e5d-27e8741fb527",
   "metadata": {},
   "outputs": [],
   "source": [
    "domains_list_grouped = domains_list.groupBy(\"domain_name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "65210b92-f1e8-4152-a340-4884579c87f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.group.GroupedData at 0x7fef903e3c10>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "domains_list_grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2098524c-e323-4f20-8de0-a886690f0039",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
