{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a36f5177-2409-429a-ace7-6a5e458c9757",
   "metadata": {},
   "source": [
    "# Connecting Social Media via Domain Names\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "--describe your project at a high level--\n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0314442c-e30d-430f-a6d3-caad3d8ee70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T\n",
    "import modules.parsers as parsers\n",
    "import glob\n",
    "import pandas as pd\n",
    "import os\n",
    "import pathlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98cbeb06-d1f3-463f-9fcd-ec5461f701c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/19 14:34:15 WARN Utils: Your hostname, pop-os resolves to a loopback address: 127.0.1.1; using 192.168.2.99 instead (on interface eno1)\n",
      "22/04/19 14:34:15 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Ivy Default Cache set to: /home/paul/.ivy2/cache\n",
      "The jars for the packages stored in: /home/paul/.ivy2/jars\n",
      "org.postgresql#postgresql added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-21d9147e-c0df-49b7-9ddf-72d2d1e04f6b;1.0\n",
      "\tconfs: [default]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/home/paul/Projects/DataEngineering/Capstone/env/lib/python3.8/site-packages/pyspark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tfound org.postgresql#postgresql;42.1.1 in central\n",
      ":: resolution report :: resolve 65ms :: artifacts dl 2ms\n",
      "\t:: modules in use:\n",
      "\torg.postgresql#postgresql;42.1.1 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   1   |   0   |   0   |   0   ||   1   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-21d9147e-c0df-49b7-9ddf-72d2d1e04f6b\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 1 already retrieved (0kB/6ms)\n",
      "22/04/19 14:34:16 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder\\\n",
    "                     .config(\"spark.jars.packages\",\"org.postgresql:postgresql:42.1.1\")\\\n",
    "                     .master(\"spark://pop-os.localdomain:7077\")\\\n",
    "                     .enableHiveSupport()\\\n",
    "                     .getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad25a579-9b1f-48df-96cb-8234800d1189",
   "metadata": {},
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "## Who is going to use the data model, think of user persona that you think will benefit from this?\n",
    "\n",
    "There are already startups out there harvesting email addresse from github repositories in order to sell people stuff.[Source](https://news.ycombinator.com/item?id=30977883) Connecting those email addresses with other social media data allows for better valuation of the value that can be extracted from each email address.\n",
    "\n",
    "For me personally I want to better understand the relationships between individuals and the groups they attach themselves to and how they overlap. For example I find a corperate entity that is capable of using the same domain name for their website as well as their email has a certain level of competence that should be noted.\n",
    "\n",
    "\n",
    "## What are that types of questions this data model can help answer?\n",
    "\n",
    "* Can we find the same domain name being shared for both email addresses and normal websites\n",
    "* Total commits per domain name, total commits per email address, total commits per git organization, most contributors per git organization, most contributors per repo \n",
    "* Can we find people using the same username across email domains as well as reddit accounts?\n",
    "\n",
    "#### Scope \n",
    "\n",
    "The purpose of this project is to develop a ETL pipeline for social media information, in this case reddit comments and git metadata. The unique quality across these datasets are domain names. Git metadata includes email addresses which use a domain name and website URL's can be extracted from reddit comments.\n",
    "\n",
    "#### Describe and Gather Data \n",
    "\n",
    "Two datasets were used for this project,\n",
    "\n",
    "* Reddit comment export from [pushift.io](https://files.pushshift.io/reddit/comments/daily/), specically the RC_2018-01-01 export\n",
    "* A custom list of git repos with metadata extracted using a custom script\n",
    "### Step 2: Explore and Assess the Data\n",
    "#### Explore the Data \n",
    "Identify data quality issues, like missing values, duplicate data, etc.\n",
    "\n",
    "**Reddit Comment**\n",
    "\n",
    "The main problem with the reddit dataset is that it is too large with various files containing JSON files 1-5 Gb in size. I split these files up to make ingestion easier.\n",
    "* Cross dataset comparisons\n",
    "  * Join domain name of git email with domain name of URL in reddit comment\n",
    "  * Join username from git email with reddit username\n",
    "* Inner dataset queries\n",
    "  * Reddit\n",
    "    * Most comments per user\n",
    "    * Most comments per subreddit\n",
    "  * Git\n",
    "    * Groupby Email Address\n",
    "    * Group by email address AND repo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85844071-125c-4707-9ab2-7d259387935c",
   "metadata": {},
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "#### Explore the Data \n",
    "Identify data quality issues, like missing values, duplicate data, etc.\n",
    "\n",
    "**Reddit Comment**\n",
    "\n",
    "The domain names inside the body of text are not parsed yet therefore I will have to do that myself.\n",
    "\n",
    "**Git data**\n",
    "\n",
    "I surveyed a lot of different tools to extract metadata from git repos. I discovered [mergestat](https://github.com/mergestat/mergestat) a simple open source program that allows one to perform SQL queries on all data in a git repository and even includes a ndjson export therefore the data can be ingested strait into pandas without a second thought.\n",
    "\n",
    "#### Cleaning Steps\n",
    "\n",
    "**Reddit Comment**\n",
    "\n",
    "There are a bunch of deleted posts that don't serve any purpose so those can be gotten rid of.\n",
    "\n",
    "**Git data**\n",
    "\n",
    "There was a problem with mergestat though. There was no way to get the remote_url of the git repo while querying SQL. To solve this I extracted the remote URL myself and fed in a hard coded column into the SQL query. To see how I solved this check out modules/gitindexer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eafe1a80-80b4-4231-a016-df882b8b0877",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performing cleaning tasks here\n",
    "# Reddit data can be loaded into spark directly\n",
    "p = pathlib.Path(\"./data/cloned-repos\")\n",
    "p.mkdir(parents=True, exist_ok=True)\n",
    "remote_urls = pd.read_json(\"./data/remote_urls_out/part-00000-0e2c3003-35ff-4f6a-825a-6bf7d5966903-c000.json\", lines=True)\n",
    "for remote_url in remote_urls.iterrows():\n",
    "    print(remote_url[1][\"remote_url\"])\n",
    "    git_clone_command = \"cd ./data/cloned-repos && git clone {}\".format(repo_url)\n",
    "    os.system(reddit_split_command) # subprocess does not always work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "53863f94-de96-438a-96cd-7de43e6d6f36",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "reddit_df = spark.read.json(\"./data/RC_2018-01-01\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d7d1e4b1-25f8-4383-a19a-7ad6772ae043",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "git_repo_df = spark.read.json(\"./data/git_out/commits/*/*.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "702f3c52-e22f-476e-ac09-8bd806d51090",
   "metadata": {},
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "Build the data pipelines to create the data model.\n",
    "\n",
    "I used a many to many schema. I have two fact tables reddit_df and git_repo_df that can be joined in two ways. Using domain names and usernames hence the two dimmension table joined_users joined_domains.\n",
    "\n",
    "**reddit_df**\n",
    "\n",
    "| Field Name             | Datatype | Constraint | Description                                            |\n",
    "|------------------------|----------|------------|--------------------------------------------------------|\n",
    "| author                 | text     | None       | Reddit username of who posted comment                  |\n",
    "| author_cakeday         | bool     | None       | True if user created account on same day previous year |\n",
    "| author_flair_css_class | text     | None       | CSS class to stylize username                          |\n",
    "| author_flair_text      | text     | None       | Custom text displayed next to username in comment      |\n",
    "| body                   | text     | None       | Contents of text                                       |\n",
    "| controversiality       | int      | None       | Score used to rank comment                             |\n",
    "| created_utc            | int      | None       | Timestamp when posted                                  |\n",
    "| edited                 | int      | None       | Timestamp when edited                                  |\n",
    "| id                     | text     | None       | Unique ID to relate to other comments                  |\n",
    "| link_id                | text     | None       | Formatted ID to relate to other comments               |\n",
    "| parent_id              | text     | None       | link_id that this comment was replying to              |\n",
    "| permalink              | text     | None       | path to comment to share                               |\n",
    "| subreddit              | text     | None       | Group comment was posted in                            |\n",
    "| subreddit_type         | text     | None       | Type of subreddit                                      |\n",
    "| url                    | text     | None       | URL extracted from body                                |\n",
    "| domain_name            | text     | None       | Domain name extracted from URL                         |\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**git_repo_df**\n",
    "\n",
    "| Field Name     | Datatype | Constraint  | Description                             |\n",
    "|----------------|----------|-------------|-----------------------------------------|\n",
    "| author_email   | text     | None        | Email of person who commited to repo    |\n",
    "| author_name    | text     | None        | Name of person who commmited to repo    |\n",
    "| commits        | int      | None        | Number of commits in this repo          |\n",
    "| remote_url     | int      | None        | Remote URL to clone this repo           |\n",
    "| email_username | int      | None        | Parsed username from this repo          |\n",
    "| email_domain   | text     | None        | Parsed domain name from this repo       |\n",
    "\n",
    "**joined_users**\n",
    "\n",
    "| Field Name     | Datatype | Constraint | Description                                    |\n",
    "|----------------|----------|------------|------------------------------------------------|\n",
    "| author_email   | text     | None       | Email address from git repo                    |\n",
    "| email_username | text     | None       | Matching username between reddit and git email |\n",
    "| author         | text     | None       | Name of person who commited to git repo        |\n",
    "\n",
    "\n",
    "**joined_domains**\n",
    "\n",
    "| Field Name          | Datatype | Constraint | Description                                                     |\n",
    "|---------------------|----------|------------|-----------------------------------------------------------------|\n",
    "| domain_name         | text     | None       | Matching domain name between reddit and git repo email address |\n",
    "| reddit_domain_count | text     | None       | Number of times this domain appeared in comments                |\n",
    "| email_domain_count  | text     | text       | Number of times this domain appeared in email addresses         |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e161c650-d6c8-4db5-874d-f46b423ad295",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions for UDFs\n",
    "# Source https://stackoverflow.com/questions/839994/extracting-a-url-in-python\n",
    "def extract_urls(s):\n",
    "    import re\n",
    "    results = re.findall(\"(?P<url>https?://[^\\s]+)\", s)\n",
    "    if results == []:\n",
    "        return \"NONE\"\n",
    "    else:\n",
    "        return results[0]\n",
    "udf_extract_urls = F.udf(extract_urls, T.StringType())\n",
    "    \n",
    "def extract_domain_name(s):\n",
    "    import re\n",
    "    results = re.findall('^(?:http:\\/\\/|www\\.|https:\\/\\/)([^\\/]+)', s)\n",
    "    if results == []:\n",
    "        return \"NONE\"\n",
    "    else:\n",
    "        return results[0]\n",
    "udf_extract_domain_name = F.udf(extract_domain_name, T.StringType())\n",
    "\n",
    "def extract_user_email(s):\n",
    "    import re\n",
    "    try:\n",
    "        results = re.findall('([^@]+)', s)\n",
    "        if results == []:\n",
    "            return \"NONE\"\n",
    "        else:\n",
    "            return results[0]\n",
    "    except:\n",
    "        return \"NONE\"\n",
    "udf_extract_user_email = F.udf(extract_user_email, T.StringType())\n",
    "\n",
    "def extract_domain_name_email(s):\n",
    "    import re\n",
    "    try:\n",
    "        results = re.findall('@(.*)', s)\n",
    "        if results == []:\n",
    "            return \"NONE\"\n",
    "        else:\n",
    "            return results[0]\n",
    "    except:\n",
    "        return \"NONE\"\n",
    "udf_extract_domain_name_email = F.udf(extract_domain_name_email, T.StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d8a772bc-878c-4b9e-8641-9489a82ed2fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_df = reddit_df.withColumn(\"url\", udf_extract_urls(reddit_df.body))\n",
    "reddit_df = reddit_df.withColumn(\"domain_name\", udf_extract_domain_name(reddit_df.url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f1d6a1b4-b5c2-46a2-8e9b-9ec619ed537a",
   "metadata": {},
   "outputs": [],
   "source": [
    "git_repo_df = git_repo_df.withColumn(\"email_username\", udf_extract_user_email(git_repo_df.author_email))\n",
    "git_repo_df = git_repo_df.withColumn(\"email_domain\", udf_extract_domain_name_email(git_repo_df.author_email))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a4cebf2e-9043-4688-af5d-31106b4aa60d",
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_domains_grouped = reddit_df.filter( F.col(\"url\") != \"NONE\" ).groupBy(\"domain_name\").count()\n",
    "reddit_domains_grouped = reddit_domains_grouped.withColumnRenamed(\"count\", \"reddit_domain_count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ef4db697-4f02-4d04-b5c2-89dcd94ff590",
   "metadata": {},
   "outputs": [],
   "source": [
    "git_domains_grouped = git_repo_df.groupBy(\"email_domain\").count()\n",
    "git_domains_grouped = git_domains_grouped.withColumnRenamed(\"count\", \"email_domain_count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dec49b02-e92a-4652-87fa-4c63753f34d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_domains = reddit_domains_grouped.alias(\"reddit\")\\\n",
    "  .join(git_domains_grouped.alias(\"git\"), F.col(\"git.email_domain\") == F.col(\"reddit.domain_name\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "009f7731-1480-4383-9fc0-4f96acf61448",
   "metadata": {},
   "outputs": [],
   "source": [
    "git_domains_grouped = git_domains_grouped.withColumnRenamed(\"email_domain\", \"domain_name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2bfac40a-c5b4-444d-bbf8-923e1cb8a327",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coppied below from Stack Overflow because spark 2.4 does not have unionByName\n",
    "# reddit_domains_grouped.unionByName( git_domains_grouped,  allowMissingColumns=True ).show()\n",
    "diff1 = [c for c in reddit_domains_grouped.columns if c not in git_domains_grouped.columns]\n",
    "diff2 = [c for c in git_domains_grouped.columns if c not in reddit_domains_grouped.columns]\n",
    "comapre_domain_df = git_domains_grouped.select('*', *[F.lit(None).alias(c) for c in diff1]) \\\n",
    "    .unionByName(reddit_domains_grouped.select('*', *[F.lit(None).alias(c) for c in diff2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "22941050-4704-4b6f-a5f1-e39b31c95fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_users = reddit_df.alias(\"reddit\")\\\n",
    "  .join(git_repo_df.alias(\"git\"), F.col(\"git.email_username\") == F.col(\"reddit.author\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "22382dfc-88d2-4875-85a5-497a657f9a61",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author_email</th>\n",
       "      <th>email_username</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>jungans@gmail.com</td>\n",
       "      <td>jungans</td>\n",
       "      <td>jungans</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mqduck@mqduck.net</td>\n",
       "      <td>mqduck</td>\n",
       "      <td>mqduck</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mqduck@mqduck.net</td>\n",
       "      <td>mqduck</td>\n",
       "      <td>mqduck</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mithrandi@mithrandi.net</td>\n",
       "      <td>mithrandi</td>\n",
       "      <td>mithrandi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>jsh@fb.com</td>\n",
       "      <td>jsh</td>\n",
       "      <td>jsh</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              author_email email_username     author\n",
       "0        jungans@gmail.com        jungans    jungans\n",
       "1        mqduck@mqduck.net         mqduck     mqduck\n",
       "2        mqduck@mqduck.net         mqduck     mqduck\n",
       "3  mithrandi@mithrandi.net      mithrandi  mithrandi\n",
       "4               jsh@fb.com            jsh        jsh"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joined_users[[\"git.author_email\", \"git.email_username\", \"reddit.author\"]].limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b47c4b-ffe7-40ed-a418-acc4ce2ab4ae",
   "metadata": {},
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "Explain the data quality checks you'll perform to ensure the pipeline ran as expected. These could include:\n",
    " * Integrity constraints on the relational database (e.g., unique key, data type, etc.)\n",
    " * Unit tests for the scripts to ensure they are doing the right thing\n",
    " * Source/Count checks to ensure completeness\n",
    " \n",
    "Run Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "af1f2931-d784-4801-a312-65d426d72b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_df_over_million_lines(df):\n",
    "    if df.count() >= 1000000:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "def contains_n_distinct_in_column(n, df, col_name):\n",
    "    if df[[col_name]].distinct().count() >= n:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e536b0dc-dd17-47c5-8bae-e089b8b798a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_df_over_million_lines(reddit_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "693bbcda-2989-4eda-b5c4-7255be9ead72",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contains_n_distinct_in_column(100, reddit_df, \"domain_name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bb98e892-bdb8-426d-96ba-7edb68f3c70e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contains_n_distinct_in_column(100, git_repo_df, \"email_domain\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "438a0d38-1714-4ca3-96e5-fbdbeebde79f",
   "metadata": {},
   "source": [
    "#### 4.3 Data dictionary \n",
    "Create a data dictionary for your data model. For each field, provide a brief description of what the data is and where it came from. You can include the data dictionary in the notebook or in a separate file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "492c16e2-b3fd-4b68-ac34-71f1bdf9eb6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Contains one column, remote_url that is a git remote url that can be used to clone the git \n",
    "repos involved in this dataset\n",
    "\"\"\"\n",
    "os.system(\"rm -rf ./data/remote_urls_out\")\n",
    "git_repo_df[[\"remote_url\"]].distinct().write.json(\"./data/remote_urls_out\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d094015-4418-4290-a96d-b000440d9410",
   "metadata": {},
   "source": [
    "**joined_domains**\n",
    "\n",
    "| Field Name          | Datatype | Constraint | Description                                                     |\n",
    "|---------------------|----------|------------|-----------------------------------------------------------------|\n",
    "| domain_name         | text     | None       | Matching domain name between reddit and git repo email address |\n",
    "| reddit_domain_count | text     | None       | Number of times this domain appeared in comments                |\n",
    "| email_domain_count  | text     | text       | Number of times this domain appeared in email addresses         |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "54fa07fe-a2a0-49b8-9e70-29dce2e17eec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "joined_domains[[\"domain_name\", \"reddit_domain_count\", \"email_domain_count\"]].write.json(\"./data/joined_domains.parquet\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "694d7fa2-75c7-4ba8-b379-e1c3cc864e41",
   "metadata": {},
   "source": [
    "**joined_users**\n",
    "\n",
    "| Field Name     | Datatype | Constraint | Description                                    |\n",
    "|----------------|----------|------------|------------------------------------------------|\n",
    "| author_email   | text     | None       | Email address from git repo                    |\n",
    "| email_username | text     | None       | Matching username between reddit and git email |\n",
    "| author         | text     | None       | Name of person who commited to git repo        |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1b344cb0-d7e3-4b91-a7e5-4fda99682e8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "joined_users[[\"git.author_email\", \"git.email_username\", \"reddit.author\"]].write.json(\"./data/joined_users.parquet\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df8c705f-36a2-4b4f-b7d2-102e5d2aad20",
   "metadata": {},
   "source": [
    "**reddit_df**\n",
    "\n",
    "| Field Name             | Datatype | Constraint | Description                                            |\n",
    "|------------------------|----------|------------|--------------------------------------------------------|\n",
    "| author                 | text     | None       | Reddit username of who posted comment                  |\n",
    "| author_cakeday         | bool     | None       | True if user created account on same day previous year |\n",
    "| author_flair_css_class | text     | None       | CSS class to stylize username                          |\n",
    "| author_flair_text      | text     | None       | Custom text displayed next to username in comment      |\n",
    "| body                   | text     | None       | Contents of text                                       |\n",
    "| controversiality       | int      | None       | Score used to rank comment                             |\n",
    "| created_utc            | int      | None       | Timestamp when posted                                  |\n",
    "| edited                 | int      | None       | Timestamp when edited                                  |\n",
    "| id                     | text     | None       | Unique ID to relate to other comments                  |\n",
    "| link_id                | text     | None       | Formatted ID to relate to other comments               |\n",
    "| parent_id              | text     | None       | link_id that this comment was replying to              |\n",
    "| permalink              | text     | None       | path to comment to share                               |\n",
    "| subreddit              | text     | None       | Group comment was posted in                            |\n",
    "| subreddit_type         | text     | None       | Type of subreddit                                      |\n",
    "| url                    | text     | None       | URL extracted from body                                |\n",
    "| domain_name            | text     | None       | Domain name extracted from URL                         |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a5b4ca01-2d84-4da0-8119-b30f535b138c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "reddit_df.write.json(\"./data/reddit_df.parquet\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d69b0a0-c70a-48ca-8616-33d46cad128f",
   "metadata": {},
   "source": [
    "**git_metadata**\n",
    "\n",
    "| Field Name     | Datatype | Constraint  | Description                             |\n",
    "|----------------|----------|-------------|-----------------------------------------|\n",
    "| author_email   | text     | None        | Email of person who commited to repo    |\n",
    "| author_name    | text     | None        | Name of person who commmited to repo    |\n",
    "| commits        | int      | None        | Number of commits in this repo          |\n",
    "| remote_url     | int      | None        | Remote URL to clone this repo           |\n",
    "| email_username | int      | None        | Parsed username from this repo          |\n",
    "| email_domain   | text     | None        | Parsed domain name from this repo       |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cef2455-0f54-40d3-aa87-f5af366fe925",
   "metadata": {},
   "source": [
    "**joined_domains**\n",
    "\n",
    "| Field Name          | Datatype | Constraint | Description                                                     |\n",
    "|---------------------|----------|------------|-----------------------------------------------------------------|\n",
    "| domain_name         | text     | None       | Matching domain name between reddit and git repo email address |\n",
    "| reddit_domain_count | text     | None       | Number of times this domain appeared in comments                |\n",
    "| email_domain_count  | text     | text       | Number of times this domain appeared in email addresses         |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e247fd-9716-4dd6-b9f1-7ec18c2d5ede",
   "metadata": {},
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "* Clearly state the rationale for the choice of tools and technologies for the project.\n",
    "  In the end I used Spark and my local file system but I had processed the same data into a variety of other data engineering tools before landing on this solution. \n",
    "  \n",
    "  Initally Spark would hang when using the regexp_extract function on the 2.3 million rows of the reddit dataset. Since I was working with Regex to process my data I tried using the regex on redshift but the [REGEXP_SUBSTR function](https://docs.aws.amazon.com/redshift/latest/dg/REGEXP_SUBSTR.html) with no success as the dialect of regex refshift supports did not have the features to process the text I needed. I then practiced loading the data and processing it in postgres where I was sucessful using the regexp_matches function. I was able to load the entire dataset into postgres and realized it would take 15-30 second to perform the regex transformation on the body column of the reddit_comments table while spark does it in about 3.\n",
    "  I also had spark setup to use S3 Object storage but over the course of troubleshooting I ended up running of my free 20000 requests on the AWS free tier.\n",
    "* Propose how often the data should be updated and why.\n",
    "  * The reddit dataset could ideally be updated in real time live from reddit.\n",
    "  * The git repo dataset should be updated with new git repos. This can be done via airflow job once a day\n",
    "  * New git repos can be added and run once a day via airflow job\n",
    "  * The aggregations table should be updated when new git repos and commits are added once a day\n",
    "* Write a description of how you would approach the problem differently under the following scenarios:\n",
    " * The data was increased by 100x.\n",
    "   * With 100x the data I would break the parsing of data into chunks. The Pushift dataset is partitioned by day therefore separate spark workers can process each piece of data into the datawarehouse. The task of scraping, indexing, and updating git repos can be paritioned in a similar way as well.\n",
    "   * With 100x more storage I would need to  data warehouse such as redshift or [Snowflake](https://www.snowflake.com/). \n",
    " * The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    "   * Since both the reddit and git data are all timestamped\n",
    " * The database needed to be accessed by 100+ people.\n",
    "   * Permissions and logging would need to be added to the database. Can't have one person accidentally writing a infinite loop that keeps endlessly requesting data from the database. Logging is imporant to find when this is happening."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
