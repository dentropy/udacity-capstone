{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Project Title\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "--describe your project at a high level--\n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Do all imports and installs here\n",
    "import pandas as pd\n",
    "import psycopg2\n",
    "import glob\n",
    "import json\n",
    "import pathlib\n",
    "import os\n",
    "import subprocess\n",
    "import pandas as pd\n",
    "from sql_queries import reddit_comments_table_create, git_metadata_table_create, list_table_names, reddit_comments_table_insert\n",
    "from modules.gitindexer import index_git_repos, get_repo_metadata_remote, get_repo_metadata_path\n",
    "import modules.parsers as parsers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "psql postgresql://user:user@localhost:5432/postgres\n"
     ]
    }
   ],
   "source": [
    "# We will use postgres\n",
    "DB_USER=\"user\"\n",
    "DB_PASSWORD=\"user\"\n",
    "HOST=\"localhost\"\n",
    "DB_PORT=5432\n",
    "DB_NAME=\"postgres\"\n",
    "aws_access_key_id = \"\"\n",
    "aws_secret_access_key = \"\"\n",
    "psql_conn_string=\"postgresql://{}:{}@{}:{}/{}\".format(DB_USER, DB_PASSWORD, HOST, DB_PORT,DB_NAME)\n",
    "print(\"psql \" + psql_conn_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = psycopg2.connect(psql_conn_string)\n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope \n",
    "\n",
    "The purpose of this project is to develop a ETL pipeline for social media information, in this case reddit comments and git metadata. The unique quality across these datasets are domain names. Git metadata includes email addresses which use a domain name and website URL's can be extracted from reddit comments.\n",
    "\n",
    "#### Describe and Gather Data \n",
    "\n",
    "Two datasets were used for this project,\n",
    "\n",
    "* Reddit comment export from [pushift.io](https://files.pushshift.io/reddit/comments/daily/), specically the RC_2018-01-01 export\n",
    "* A custom list of git repos with metadata extracted using a custom script\n",
    "\n",
    "* Cross dataset comparisons\n",
    "  * Join domain name of git email with domain name of URL in reddit comment\n",
    "  * Join username from git email with reddit username\n",
    "* Inner dataset queries\n",
    "  * Reddit\n",
    "    * Most comments per user\n",
    "    * Most comments per subreddit\n",
    "  * Git\n",
    "    * Groupby Email Address\n",
    "    * Group by email address AND repo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "#### Explore the Data \n",
    "Identify data quality issues, like missing values, duplicate data, etc.\n",
    "\n",
    "**Reddit Comment**\n",
    "\n",
    "The main problem with the reddit dataset is that it is too large with various files containing JSON files 1-5 Gb in size. I split these files up to make ingestion easier.\n",
    "\n",
    "**Git data**\n",
    "\n",
    "I surveyed a lot of different tools to extract metadata from git repos. I discovered [mergestat](https://github.com/mergestat/mergestat) a simple open source program that allows one to perform SQL queries on all data in a git repository and even includes a ndjson export therefore the data can be ingested strait into pandas without a second thought.\n",
    "\n",
    "#### Cleaning Steps\n",
    "\n",
    "**Reddit Comment**\n",
    "\n",
    "I had to create a schema for the reddit comments and I did not have a use for some of the key value pairs. I did not want to import all the key value pairs within the export therefore I selected a subset of columns from the dataframe.\n",
    "\n",
    "**Git data**\n",
    "\n",
    "There was a problem with mergestat though. There was no way to get the remote_url of the git repo while querying SQL. To solve this I extracted the remote URL myself and fed in a hard coded column into the SQL query. To see how I solved this check out modules/gitindexer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Performing cleaning tasks here\n",
    "# Split large ndjson file with reddit comments\n",
    "\n",
    "p = pathlib.Path(\"./data/reddit_split\")\n",
    "p.mkdir(parents=True, exist_ok=True)\n",
    "reddit_split_command = \"cd data && split -l 1000 RC_2018-01-01 reddit_split/reddit_2018_01_01_\"\n",
    "# process = subprocess.Popen(reddit_split_command, shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load reddit data into postgres\n",
    "\n",
    "parsers.drop_tables(psql_conn_string, list_table_names)\n",
    "parsers.drop_tables(psql_conn_string, [\"reddit_comments2\", \"test_reddit_comments\"])\n",
    "sub_df = [\"author\", \"body\", \"score\", \"can_gild\", \"controversiality\", \"created_utc\", \"edited\", \"id\", \"is_submitter\", \"link_id\", \"parent_id\", \"permalink\", \"subreddit\"]\n",
    "# parsers.json_glob_to_database(\"./data/reddit_split/**\", psql_conn_string, \"reddit_comments\", sub_df)\n",
    "\n",
    "# TESTING\n",
    "parsers.json_glob_to_database(\"./data/reddit_split/reddit_2018_01_01_bf\", psql_conn_string, \"test_reddit_comments\", sub_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone git repos\n",
    "\n",
    "p = pathlib.Path(\"./data/cloned-repos\")\n",
    "p.mkdir(parents=True, exist_ok=True)\n",
    "git_repo_urls = json.load(open(\"./data/git_repos.json\"))\n",
    "for repo_url in git_repo_urls:\n",
    "    git_clone_command = \"cd ./data/cloned-repos && git clone {}\".format(repo_url)\n",
    "    # os.system(reddit_split_command) # subprocess does not always work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get git repo metadata\n",
    "git_paths = glob.glob(\"./data/cloned-repos/**\",)\n",
    "for git_path in git_paths:\n",
    "    # print(\"Indexing\", git_path)\n",
    "    try:\n",
    "        pass\n",
    "        # get_repo_metadata_path(git_path, \"commits\", \"./data/git_out/\")\n",
    "    except:\n",
    "        print(\"Got an error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load git_metadata into postgres\n",
    "\n",
    "parsers.drop_tables(psql_conn_string, [\"git_metadata\", \"git_metadata2\", \"test_git_metadata\"])\n",
    "# parsers.json_glob_to_database(\"./data/git_out/*/*/*.json\", psql_conn_string, \"git_metadata\")\n",
    "\n",
    "# TESTING\n",
    "parsers.json_glob_to_database(\"./data/git_out/commits/livepeer/livepeerjs.json\", psql_conn_string, \"test_git_metadata\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "\n",
    "We have two fact tables. One fact table is git_metadata, number of commits, email, remote_url. The other fact table is reddit_comments which contains the author, subreddit, time of submisson etc. etc.\n",
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "\n",
    "There are two separate data pipelines, one for each fact table. Then dimmension tables can be created to store joins, aggregation and other results.\n",
    "\n",
    "Rather than store the domain name in a separate table I wrote a function that adds a column and performs a regex query in order to extract the data I desired.\n",
    "\n",
    "**git_metadata fact talbe**\n",
    "\n",
    "1. Load data into database\n",
    "  * For this I just read all the processed JSON files using glob then load each one of them into the SQL database\n",
    "2. Add email_username column, update column with username from author_email column using regex\n",
    "3. Add email_domain column, update column with username from author_email column using regex\n",
    "\n",
    "**reddit_comments fact table**\n",
    "\n",
    "1. Load data into database\n",
    "2. Add url column, update column with url from body column using regex\n",
    "3. Add domainname column, update column with domainname from url column  using regex\n",
    "\n",
    "**shared_domains dimmension table**\n",
    "\n",
    "## TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "Build the data pipelines to create the data model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Write code here\n",
    "\n",
    "parsers.sql_extract_url(psql_conn_string, \"test_reddit_comments\", \"body\", \"url\")\n",
    "parsers.sql_parse_email_domainname(psql_conn_string, \"test_git_metadata\", \"author_email\", \"email_domain\")\n",
    "parsers.sql_extract_domain_from_url_regex(psql_conn_string, \"test_reddit_comments\", \"url\", \"domainname\")\n",
    "parsers.sql_parse_email_username(psql_conn_string, \"test_git_metadata\", \"author_email\", \"email_username\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parsers.sql_extract_url(\"reddit_comments\", \"body\", \"url\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sql_parse_email_domainname(\"git_metadata\", \"author_email\", \"email_domain\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sql_parse_email_username(\"git_metadata\", \"author_email\", \"email_username\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sql_extract_domain_from_url_regex(\"reddit_comments\", \"url\", \"domainname\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "Explain the data quality checks you'll perform to ensure the pipeline ran as expected. These could include:\n",
    " * Integrity constraints on the relational database (e.g., unique key, data type, etc.)\n",
    " * Unit tests for the scripts to ensure they are doing the right thing\n",
    " * Source/Count checks to ensure completeness\n",
    " \n",
    "Run Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Perform quality checks here\n",
    "\n",
    "# Check total counts on all tables\n",
    "# Check for primary key count on each table\n",
    "# Check distinct domain_names on all tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.3 Data dictionary \n",
    "Create a data dictionary for your data model. For each field, provide a brief description of what the data is and where it came from. You can include the data dictionary in the notebook or in a separate file.\n",
    "\n",
    "**Check \"Describe and Gather Data\" section in README and above**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "* Clearly state the rationale for the choice of tools and technologies for the project.\n",
    "  * I used postgres because spark kept crashing when I loaded the reddit data into it.\n",
    "* Propose how often the data should be updated and why.\n",
    "  * \n",
    "* Write a description of how you would approach the problem differently under the following scenarios:\n",
    " * The data was increased by 100x.\n",
    "   * I can not use redshift because I perform a regex transformation it does not support, I know this because I tested it. A traditioanl postgres database with terabytes of storage would work for 100x larger dataset. Though larger than that I would want to use the [Snowflake](https://www.snowflake.com/) data lake / data warehouse and take advantage of its [PARSE_URL â€” Snowflake Documentation](https://docs.snowflake.com/en/sql-reference/functions/parse_url.html) function \n",
    " * The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    "   * No dashboard, but I can build one\n",
    " * The database needed to be accessed by 100+ people."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
