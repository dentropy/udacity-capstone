{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Connecting Social Media via Domain Names\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "--describe your project at a high level--\n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Do all imports and installs here\n",
    "import pandas as pd\n",
    "import psycopg2\n",
    "import glob\n",
    "import json\n",
    "import pathlib\n",
    "import os\n",
    "import subprocess\n",
    "import pandas as pd\n",
    "from sql_queries import reddit_comments_table_create, git_metadata_table_create, list_table_names, reddit_comments_table_insert\n",
    "from modules.gitindexer import index_git_repos, get_repo_metadata_remote, get_repo_metadata_path\n",
    "import modules.parsers as parsers\n",
    "import configparser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use postgres\n",
    "config = configparser.ConfigParser()\n",
    "config.sections()\n",
    "config.read('dl.cfg')\n",
    "DB_USER=    config['POSTGRES']['DB_USER']\n",
    "DB_PASSWORD=config['POSTGRES']['DB_PASSWORD']\n",
    "DB_HOST=    config['POSTGRES']['DB_HOST']\n",
    "DB_PORT=    config['POSTGRES']['DB_PORT']\n",
    "DB_NAME=    config['POSTGRES']['DB_NAME']\n",
    "psql_conn_string=\"postgresql://{}:{}@{}:{}/{}\".format(DB_USER, DB_PASSWORD, DB_HOST, DB_PORT,DB_NAME)\n",
    "# print(\"psql \" + psql_conn_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = psycopg2.connect(psql_conn_string)\n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "tags": []
   },
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope \n",
    "\n",
    "The purpose of this project is to develop a ETL pipeline for social media information, in this case reddit comments and git metadata. The unique quality across these datasets are domain names. Git metadata includes email addresses which use a domain name and website URL's can be extracted from reddit comments.\n",
    "\n",
    "#### Describe and Gather Data \n",
    "\n",
    "Two datasets were used for this project,\n",
    "\n",
    "* Reddit comment export from [pushift.io](https://files.pushshift.io/reddit/comments/daily/), specically the RC_2018-01-01 export\n",
    "* A custom list of git repos with metadata extracted using a custom script\n",
    "\n",
    "* Cross dataset comparisons\n",
    "  * Join domain name of git email with domain name of URL in reddit comment\n",
    "  * Join username from git email with reddit username\n",
    "* Inner dataset queries\n",
    "  * Reddit\n",
    "    * Most comments per user\n",
    "    * Most comments per subreddit\n",
    "  * Git\n",
    "    * Groupby Email Address\n",
    "    * Group by email address AND repo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "#### Explore the Data \n",
    "Identify data quality issues, like missing values, duplicate data, etc.\n",
    "\n",
    "**Reddit Comment**\n",
    "\n",
    "The main problem with the reddit dataset is that it is too large with various files containing JSON files 1-5 Gb in size. I split these files up to make ingestion easier.\n",
    "\n",
    "**Git data**\n",
    "\n",
    "I surveyed a lot of different tools to extract metadata from git repos. I discovered [mergestat](https://github.com/mergestat/mergestat) a simple open source program that allows one to perform SQL queries on all data in a git repository and even includes a ndjson export therefore the data can be ingested strait into pandas without a second thought.\n",
    "\n",
    "#### Cleaning Steps\n",
    "\n",
    "**Reddit Comment**\n",
    "\n",
    "I had to create a schema for the reddit comments and I did not have a use for some of the key value pairs. I did not want to import all the key value pairs within the export therefore I selected a subset of columns from the dataframe.\n",
    "\n",
    "**Git data**\n",
    "\n",
    "There was a problem with mergestat though. There was no way to get the remote_url of the git repo while querying SQL. To solve this I extracted the remote URL myself and fed in a hard coded column into the SQL query. To see how I solved this check out modules/gitindexer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Performing cleaning tasks here\n",
    "# Split large ndjson file with reddit comments\n",
    "\n",
    "p = pathlib.Path(\"./data/reddit_split\")\n",
    "p.mkdir(parents=True, exist_ok=True)\n",
    "reddit_split_command = \"cd data && split -l 1000 RC_2018-01-01 reddit_split/reddit_2018_01_01_\"\n",
    "process = subprocess.Popen(reddit_split_command, shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load reddit data into postgres\n",
    "\n",
    "parsers.drop_tables(psql_conn_string, list_table_names)\n",
    "parsers.drop_tables(psql_conn_string, [\"reddit_comments2\", \"test_reddit_comments\"])\n",
    "sub_df = [\"author\", \"body\", \"score\", \"can_gild\", \"controversiality\", \"created_utc\", \"edited\", \"id\", \"is_submitter\", \"link_id\", \"parent_id\", \"permalink\", \"subreddit\"]\n",
    "parsers.json_glob_to_database(\"./data/reddit_split/**\", psql_conn_string, \"reddit_comments\", sub_df)\n",
    "\n",
    "# TESTING\n",
    "# parsers.json_glob_to_database(\"./data/reddit_split/reddit_2018_01_01_bf\", psql_conn_string, \"test_reddit_comments\", sub_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone git repos\n",
    "\n",
    "p = pathlib.Path(\"./data/cloned-repos\")\n",
    "p.mkdir(parents=True, exist_ok=True)\n",
    "git_repo_urls = json.load(open(\"./data/git_repos.json\"))\n",
    "for repo_url in git_repo_urls:\n",
    "    git_clone_command = \"cd ./data/cloned-repos && git clone {}\".format(repo_url)\n",
    "    # os.system(reddit_split_command) # subprocess does not always work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get git repo metadata\n",
    "git_paths = glob.glob(\"./data/cloned-repos/**\",)\n",
    "for git_path in git_paths:\n",
    "    # print(\"Indexing\", git_path)\n",
    "    try:\n",
    "        pass\n",
    "        # get_repo_metadata_path(git_path, \"commits\", \"./data/git_out/\")\n",
    "    except:\n",
    "        print(\"Got an error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load git_metadata into postgres\n",
    "\n",
    "parsers.drop_tables(psql_conn_string, [\"git_metadata\", \"git_metadata2\", \"test_git_metadata\"])\n",
    "parsers.json_glob_to_database(\"./data/git_out/*/*/*.json\", psql_conn_string, \"git_metadata\")\n",
    "\n",
    "# TESTING\n",
    "# parsers.json_glob_to_database(\"./data/git_out/commits/livepeer/livepeerjs.json\", psql_conn_string, \"test_git_metadata\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "\n",
    "We have two fact tables. One fact table is git_metadata, number of commits, email, remote_url. The other fact table is reddit_comments which contains the author, subreddit, time of submisson etc. etc.\n",
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "\n",
    "There are two separate data pipelines, one for each fact table. Then dimmension tables can be created to store joins, aggregation and other results.\n",
    "\n",
    "Rather than store the domain name in a separate table I wrote a function that adds a column and performs a regex query in order to extract the data I desired.\n",
    "\n",
    "**git_metadata fact talbe**\n",
    "\n",
    "1. Load data into database\n",
    "  * For this I just read all the processed JSON files using glob then load each one of them into the SQL database\n",
    "2. Add email_username column, update column with username from author_email column using regex\n",
    "3. Add email_domain column, update column with username from author_email column using regex\n",
    "\n",
    "**reddit_comments fact table**\n",
    "\n",
    "1. Load data into database\n",
    "2. Add url column, update column with url from body column using regex\n",
    "3. Add domainname column, update column with domainname from url column  using regex\n",
    "\n",
    "**shared_domains dimmension table**\n",
    "\n",
    "## TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "Build the data pipelines to create the data model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Write code here\n",
    "\n",
    "# parsers.sql_extract_url(psql_conn_string, \"test_reddit_comments\", \"body\", \"url\")\n",
    "# parsers.sql_parse_email_domainname(psql_conn_string, \"test_git_metadata\", \"author_email\", \"email_domain\")\n",
    "# parsers.sql_extract_domain_from_url_regex(psql_conn_string, \"test_reddit_comments\", \"url\", \"domainname\")\n",
    "# parsers.sql_parse_email_username(psql_conn_string, \"test_git_metadata\", \"author_email\", \"email_username\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsers.sql_extract_url(psql_conn_string, \"reddit_comments\", \"body\", \"url\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsers.sql_parse_email_domainname(psql_conn_string, \"git_metadata\", \"author_email\", \"email_domain\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsers.sql_parse_email_username(psql_conn_string, \"git_metadata\", \"author_email\", \"email_username\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsers.sql_extract_domain_from_url_regex(psql_conn_string, \"reddit_comments\", \"url\", \"domainname\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of joined domain names\n",
    "number_domain_name_query = \"\"\"\n",
    "SELECT COUNT((reddit_comments.domainname))\n",
    "    FROM reddit_comments JOIN git_metadata \n",
    "    ON reddit_comments.domainname = git_metadata.email_domain;\n",
    "\"\"\"\n",
    "number_domain_name_result = pd.read_sql_query(number_domain_name_query ,psql_conn_string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of joined domain names\n",
    "list_joined_domain_names_query = \"\"\"\n",
    "SELECT DISTINCT(reddit_comments.domainname) \n",
    "    FROM reddit_comments JOIN git_metadata \n",
    "    ON reddit_comments.domainname = git_metadata.email_domain;\n",
    "\"\"\"\n",
    "joined_domain_names_result = pd.read_sql_query(list_joined_domain_names_query ,psql_conn_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of joined usernames\n",
    "count_joined_user_names_query = \"\"\"\n",
    "SELECT COUNT(DISTINCT(reddit_comments.author))\n",
    "    FROM reddit_comments JOIN git_metadata \n",
    "    ON reddit_comments.author = git_metadata.email_username;\n",
    "\"\"\"\n",
    "count_usernames_result = pd.read_sql_query(count_joined_user_names_query ,psql_conn_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of results joining the datasets by domain name\n",
      "   count\n",
      "0  17508\n",
      "\n",
      "List of overlapping domain names\n",
      "              domainname\n",
      "0           {amazon.com}\n",
      "1               {ark.io}\n",
      "2            {augur.net}\n",
      "3          {binance.com}\n",
      "4            {bitso.com}\n",
      "5         {dogecoin.com}\n",
      "6               {dss.co}\n",
      "7   {freenetproject.org}\n",
      "8        {getmonero.org}\n",
      "9           {github.com}\n",
      "10          {golang.org}\n",
      "11          {google.com}\n",
      "12           {hackmd.io}\n",
      "13       {instagram.com}\n",
      "14             {iohk.io}\n",
      "15            {iota.org}\n",
      "16   {lightning.network}\n",
      "17        {loopring.org}\n",
      "18         {mailbox.org}\n",
      "19       {mainframe.com}\n",
      "20        {makerdao.com}\n",
      "21             {neo.org}\n",
      "22        {nextdoor.com}\n",
      "23       {pinterest.com}\n",
      "24           {prezi.com}\n",
      "25            {purse.io}\n",
      "26      {radarrelay.com}\n",
      "27            {sia.tech}\n",
      "28        {snapchat.com}\n",
      "29      {soundcloud.com}\n",
      "30         {steemit.com}\n",
      "31       {travis-ci.org}\n",
      "32          {trello.com}\n",
      "33         {unity3d.com}\n",
      "34        {uwaterloo.ca}\n",
      "\n",
      "Number of emails that have the same username as a reddit user\n",
      "   count\n",
      "0      0\n",
      "Getting no resulsts here is interesting, increasing the dataset should eventually lead to a collusion\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nNumber of results joining the datasets by domain name\")\n",
    "print(number_domain_name_result)\n",
    "print(\"\\nList of overlapping domain names\")\n",
    "print(joined_domain_names_result)\n",
    "print(\"\\nNumber of emails that have the same username as a reddit user\")\n",
    "print(count_usernames_result)\n",
    "print(\"Getting no resulsts here is interesting, increasing the dataset should eventually lead to a collusion\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "Explain the data quality checks you'll perform to ensure the pipeline ran as expected. These could include:\n",
    " * Integrity constraints on the relational database (e.g., unique key, data type, etc.)\n",
    " * Unit tests for the scripts to ensure they are doing the right thing\n",
    " * Source/Count checks to ensure completeness\n",
    " \n",
    "Run Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check total counts on all tables\n",
    "tmp_query = \"SELECT COUNT(*) FROM reddit_comments;\"\n",
    "num_reddit_comments = pd.read_sql_query(tmp_query ,psql_conn_string)\n",
    "tmp_query = \"SELECT COUNT(*) FROM git_metadata;\"\n",
    "num_git_metadata = pd.read_sql_query(tmp_query ,psql_conn_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check distinct primary key count on each table\n",
    "\n",
    "tmp_query = \"SELECT COUNT(DISTINCT(p_key)) FROM reddit_comments;\"\n",
    "num_reddit_comments_primary_key = pd.read_sql_query(tmp_query ,psql_conn_string)\n",
    "tmp_query = \"SELECT COUNT(DISTINCT(p_key)) FROM git_metadata;\"\n",
    "num_git_metadata_primary_key = pd.read_sql_query(tmp_query ,psql_conn_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Check distinct domain_names on all tables\n",
    "tmp_query = \"SELECT DISTINCT(domainname) FROM reddit_comments;\"\n",
    "num_reddit_comments_domain_names = pd.read_sql_query(tmp_query ,psql_conn_string)\n",
    "tmp_query = \"SELECT DISTINCT(email_domain) FROM git_metadata;\"\n",
    "num_git_metadata_domain_names = pd.read_sql_query(tmp_query ,psql_conn_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check distinct users on all tables\n",
    "tmp_query = \"SELECT DISTINCT(author) FROM reddit_comments;\"\n",
    "num_reddit_comments_primary_key = pd.read_sql_query(tmp_query ,psql_conn_string)\n",
    "tmp_query = \"SELECT DISTINCT(email_username) FROM git_metadata;\"\n",
    "num_reddit_comments_primary_key = pd.read_sql_query(tmp_query ,psql_conn_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of primary keys for reddit_comments matches number of rows\n"
     ]
    }
   ],
   "source": [
    "if num_reddit_comments[[\"count\"]].values[0][0] == num_reddit_comments_primary_key[[\"count\"]].values[0][0]:\n",
    "    print(\"Number of primary keys for reddit_comments matches number of rows\")\n",
    "else:\n",
    "    print(\"We got a problem\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of primary keys for reddit_comments matches number of rows\n"
     ]
    }
   ],
   "source": [
    "if num_git_metadata[[\"count\"]].values[0][0] == num_git_metadata_primary_key[[\"count\"]].values[0][0]:\n",
    "    print(\"Number of primary keys for reddit_comments matches number of rows\")\n",
    "else:\n",
    "    print(\"We got a problem\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 16844  distinct domain names from the reddit dataset\n",
      "There are 3444 distinct domain names from the git dataset\n"
     ]
    }
   ],
   "source": [
    "print(\"There are\", str(len(num_reddit_comments_domain_names)), \" distinct domain names from the reddit dataset\")\n",
    "print(\"There are\", str(len(num_git_metadata_domain_names)), \"distinct domain names from the git dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.3 Data dictionary \n",
    "Create a data dictionary for your data model. For each field, provide a brief description of what the data is and where it came from. You can include the data dictionary in the notebook or in a separate file.\n",
    "\n",
    "**Check \"Describe and Gather Data\" section in README and above**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "* Clearly state the rationale for the choice of tools and technologies for the project.\n",
    "  * I used postgres because spark kept crashing when I performed the regex query on the entire reddit data set. I then tried to use redshift's [REGEXP_SUBSTR function](https://docs.aws.amazon.com/redshift/latest/dg/REGEXP_SUBSTR.html) with no success as the dialect of regex it supports did not have the features I needed therefore I ended up using postgres with its regexp_matches function.\n",
    "* Propose how often the data should be updated and why.\n",
    "  * The reddit dataset could ideally be updated in real time live from reddit.\n",
    "  * The git repo dataset should be updated with new git repos. This can be done via airflow job once a day\n",
    "  * New git repos can be added and run once a day via airflow job\n",
    "  * The aggregations table should be updated when new git repos and commits are added once a day\n",
    "* Write a description of how you would approach the problem differently under the following scenarios:\n",
    " * The data was increased by 100x.\n",
    "   * I would use [AWS RDS](https://aws.amazon.com/rds/) to get a postgres cluster the appriproate size for my dataset. If postgres runs into scaling issues I would look into using using [Snowflake](https://www.snowflake.com/) data lake / data warehouse. Snowflake also has built in functions like  [PARSE_URL — Snowflake Documentation](https://docs.snowflake.com/en/sql-reference/functions/parse_url.html) which would make the project code more simple.\n",
    " * The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    "   * Since both the reddit and git data are all timestamped\n",
    " * The database needed to be accessed by 100+ people.\n",
    "   * Database Read Replicas can be used to make data highly available and are supported on AWS RDS."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
